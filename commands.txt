# http://s3-ap-southeast-2.amazonaws.com/ndd-reinvent-data/workshopguide.html

## Configure user credentials
# Ensure you have your credentials. If your AWS IAM user doesn't have an Access Key then you'll need to follow these steps to generate one: http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html
aws configure
# Use your existing AWS Access Key ID, AWS Secret Access Key, Default region name=us-west-2 and leave the Default output format empty (just hit Enter)

aws ec2 create-vpc --cidr-block 10.0.0.0/16
# Replace all following instances of <vpc-id> in this script with the "VpcId" from the JSON that is returned in your console window.
aws ec2 create-subnet --vpc-id <vpc-id> --cidr-block 10.0.1.0/24 --availability-zone us-west-2a
# Replace all following instances of <subnet-id1> in this script with the "SubnetId" from the JSON that is returned in your console window.
aws ec2 create-subnet --vpc-id <vpc-id> --cidr-block 10.0.2.0/24 --availability-zone us-west-2b
# Replace all following instances of <subnet-id2> in this script with the "SubnetId" from the JSON that is returned in your console window.
aws ec2 create-subnet --vpc-id <vpc-id> --cidr-block 10.0.3.0/24 --availability-zone us-west-2c
# Replace all following instances of <subnet-id3> in this script with the "SubnetId" from the JSON that is returned in your console window.

aws ec2 create-internet-gateway
# Replace all following instances of <igw-id> in this script with the "InternetGatewayId" from the JSON that is returned in your console window.

aws ec2 attach-internet-gateway --vpc-id <vpc-id> --internet-gateway-id <igw-id>

aws ec2 create-route-table --vpc-id <vpc-id>
# Replace all following instances of <route-table-id> in this script with the "RouteTableId" from the JSON that is returned in your console window.

aws ec2 create-route --route-table-id <route-table-id> --destination-cidr-block 0.0.0.0/0 --gateway-id <igw-id>

aws ec2 describe-route-tables --route-table-id <route-table-id>
# Verify route has been created correctly. There will be two routes, the first which is the default route showing that traffic destined for the 10.0.0.0/16 network will stay local, and the second of which will be the route you just added, with your "GatewayId" showing and a "DestinationCidrBlock": "0.0.0.0/0" attached to it.

aws ec2 describe-subnets --filters "Name=vpc-id,Values=<vpc-id>" --query 'Subnets[*].{ID:SubnetId,AZ:AvailabilityZone,CIDR:CidrBlock}'
# This should show 3 subnets with the CIDR blocks listed above.

## Associate your first subnet with the public route, making it a public subnet
aws ec2 associate-route-table --subnet-id <subnet-id1> --route-table-id <route-table-id>

## Enable public IP for instances created in the public subnet
aws ec2 modify-subnet-attribute --subnet-id <subnet-id1> --map-public-ip-on-launch

## Create a key pair to use for SSH access into the Bastion Host
aws ec2 create-key-pair --key-name myWorkshopKeyPair --query 'KeyMaterial' --output text > myWorkshopKeyPair.pem

## Grant the file owner (you!) read permission
chmod 400 myWorkshopKeyPair.pem

## Create security groups
aws ec2 create-security-group --group-name BastionHostAccess --description "Allow access for Bastion Host" --vpc-id <vpc-id>
# Replace all following instances of <bastion-security-group-id> in this script with the "GroupId" from the JSON that is returned in your console window.
aws ec2 create-security-group --group-name RedshiftAccess --description "Allow access for Redshift" --vpc-id <vpc-id>
# Replace all following instances of <redshift-security-group-id> in this script with the "GroupId" from the JSON that is returned in your console window.

# Add rule to allow SSH access to the Bastion Host from *anywhere*
aws ec2 authorize-security-group-ingress --group-id <bastion-security-group-id> --protocol tcp --port 22 --cidr 0.0.0.0/0
# Add rule to allow SQL access to the Redshift cluster from anywhere
# TODO IS THIS NECESSARY SEEING THAT WE GRANT ACCESS TO THE BASTION HOST BELOW??? SEEMS A BIT TOO OPEN TO THE WORLD.
aws ec2 authorize-security-group-ingress --group-id <redshift-security-group-id> --protocol tcp --port 5439 --cidr 0.0.0.0/0
# Need to enable FH access to Redshift
aws ec2 authorize-security-group-ingress --group-id <redshift-security-group-id> --protocol tcp --port 5439 --cidr 52.89.255.224/27

# Add rule to allow access to Redshift from Quicksight
aws ec2 authorize-security-group-ingress --group-id <redshift-security-group-id> --protocol tcp --port 5439 --cidr 54.70.204.128/27

## Now create the Bastion Host (based on an AMI we prepared earlier) (Nirav's bastion AMI in us-west-2 is ami-dbec22a3)
aws ec2 run-instances --image-id ami-7f2afa07 --count 1 --instance-type t2.micro --key-name myWorkshopKeyPair --security-group-ids <bastion-security-group-id> --subnet-id <subnet-id1>
# Replace all following instances of <instance-id> in this script with the "InstanceId" from the JSON that is returned in your console window.

## Get the public IP so that you can connect to it
aws ec2 describe-instances --instance-id <instance-id> --query 'Reservations[*].Instances[*].PublicIpAddress' --output=text
# Replace all following instances of <public-ip-address> in this script with the IP address that is returned in your console window.

# Add rule to allow access to Redshift from the Bastion Host
aws ec2 authorize-security-group-ingress --group-id <redshift-security-group-id> --protocol tcp --port 5439 --cidr <public-ip-address>/32

## Get into the instance
ssh -i myWorkshopKeyPair.pem ec2-user@<public-ip-address>

#############################################################################################

##Configure the AWS CLI
aws configure
# Use your existing AWS Access Key ID, AWS Secret Access Key, Default region name=us-west-2 and leave the Default output format empty (just hit Enter)

## Store your AWS Account ID as a variable
accountid="$(aws sts get-caller-identity --query "Account" --output text)"
## Create an S3 bucket for your game telemetry files
# aws s3api create-bucket --bucket "workshop-data-${accountid}" --acl private --create-bucket-configuration LocationConstraint=us-west-2
## Create an S3 bucket for your game bot
aws s3api create-bucket --bucket "workshop-bot-${accountid}" --acl private --create-bucket-configuration LocationConstraint=us-west-2

## WE NEED TO ENSURE THE JSON IS IN THEIR ROOT USER FOLDER - VERIFY WHETHER THIS IS NECESSARY
## Create a role for Redshift from the policy document provided
aws iam create-role --role-name redshift_fullaccess_role --assume-role-policy-document file://iam-base-redshift-policy.json
## Apply a policy to the role to allow full access to Redshift
aws iam put-role-policy --role-name redshift_fullaccess_role --policy-name iam-redshift-policy --policy-document file://iam-redshift-policy.json

## Create your Kinesis stream for telemetry
aws kinesis create-stream --stream-name evalTelemetryStream --shard-count 10
## Create your Kinesis stream for analytics
aws kinesis create-stream --stream-name evalAnalyticsStream --shard-count 10

## Create a role for Firehose from the policy document provided
aws iam create-role --role-name firehose_delivery_role --assume-role-policy-document file://iam-base-fh-policy.json
## Apply a policy to the role to allow full access to Firehose
aws iam put-role-policy --role-name firehose_delivery_role --policy-name iam-fh-policy --policy-document file://iam-fh-policy.json

## Create a log group and streams for telemetry
aws logs create-log-group  --log-group-name "/aws/kinesisfirehose/evalTelemetryFHStream"
aws logs create-log-stream --log-group-name "/aws/kinesisfirehose/evalTelemetryFHStream" --log-stream-name "S3Delivery"
aws logs create-log-stream --log-group-name "/aws/kinesisfirehose/evalTelemetryFHStream" --log-stream-name "RedshiftDelivery"

## Create a log group and streams for analytics
aws logs create-log-group  --log-group-name "/aws/kinesisfirehose/evalAnalyticsFHStream"
aws logs create-log-stream --log-group-name "/aws/kinesisfirehose/evalAnalyticsFHStream" --log-stream-name "S3Delivery"
aws logs create-log-stream --log-group-name "/aws/kinesisfirehose/evalAnalyticsFHStream" --log-stream-name "RedshiftDelivery"

## Create Redshift cluster
aws redshift create-cluster-subnet-group --cluster-subnet-group-name workshopsubnetgroup  --description "My subnet group for the workshop" --subnet-ids <subnet-id1> <subnet-id2> <subnet-id3>
aws redshift create-cluster --cluster-identifier examplecluster --master-username evaluser --master-user-password evalP4s^w0rd1 --cluster-type single-node --node-type ds2.xlarge --cluster-type single-node --db-name evaldb --cluster-subnet-group-name workshopsubnetgroup --vpc-security-group-ids <redshift-security-group-id> 

## Connect to the Redshift DB and create the tables in Redshift
# Update connect.sh with cluster details
./connect.sh
create table eval01table (playerip varchar(16), handle varchar(40), email varchar(128), uuid varchar(64), playerid bigint, country varchar(64), useragent varchar(128), datestamp timestamptz, walletbalance real, playerlevel varchar(16), status varchar(16));
create table eval02table (datestamp timestamptz, playerid bigint, playerlevel varchar(16), squadelementmap varchar(8), gamenumber int, squadpower int, squadagility int, squadhealth int, squadluck int, squadspecial int, squadguard int, squaddamage int, squadinventoryitemcount int, bosspower int, bossagility int, bosshealth int, bossluck int, bossspecial int, bossguard int, bossdamage int, result varchar(8));

## TODO NEED TO UPDATE THE JSON
aws firehose create-delivery-stream --delivery-stream-name evalAnalyticsFHStream --delivery-stream-type KinesisStreamAsSource --kinesis-stream-source-configuration "KinesisStreamARN=arn:aws:kinesis:us-west-2:${accountid}:stream/evalAnalyticsStream,RoleARN=arn:aws:iam::${accountid}:role/firehose_delivery_role" --cli-input-json file://analyticsInput.json

aws firehose create-delivery-stream --delivery-stream-name evalTelemetryFHStream --delivery-stream-type KinesisStreamAsSource --kinesis-stream-source-configuration "KinesisStreamARN=arn:aws:kinesis:us-west-2:${accountid}:stream/evalTelemetryStream,RoleARN=arn:aws:iam::${accountid}:role/firehose_delivery_role" --cli-input-json file://telemetryInput.json

aws firehose create-delivery-stream --delivery-stream-name directTelemetryFHStream --delivery-stream-type DirectPut --cli-input-json file://telemetryInput.json


## Cleanup
aws iam delete-role-policy --role-name redshift_fullaccess_role --policy-name iam-redshift-policy
aws iam delete-role --role-name redshift_fullaccess_role
aws iam delete-role-policy --role-name firehose_delivery_role --policy-name iam-fh-policy
aws iam delete-role --role-name firehose_delivery_role
aws logs delete-log-group --log-group-name "/aws/kinesisfirehose/evalTelemetryFHStream"
aws logs delete-log-group --log-group-name "/aws/kinesisfirehose/evalAnalyticsFHStream"
aws firehose delete-delivery-stream --delivery-stream-name evalTelemetryFHStream
aws firehose delete-delivery-stream --delivery-stream-name evalAnalyticsFHStream
aws kinesis delete-stream --stream-name evalTelemetryStream
aws kinesis delete-stream --stream-name evalAnalyticsStream

aws s3api delete-bucket --bucket kinesis-bot-datagen
# aws s3api delete-bucket --bucket --bucket "workshop-data-${aws sts get-caller-identity --query "Account" --output text}"

## QuickSight: 54.70.204.128/27


## Cleanup
#############################################################################################
## Terminate Bastion Host instance
aws ec2 terminate-instances --instance-ids <instance-id>
aws redshift delete-cluster --cluster-identifier examplecluster --skip-final-cluster-snapshot
aws redshift delete-cluster-subnet-group --cluster-subnet-group-name workshopsubnetgroup

# TODO DELETE KEY PAIR, S3 buckets

## Iterate for more than 1 resource, as required
aws ec2 delete-security-group --group-id <bastion-security-group-id>
aws ec2 delete-security-group --group-id <redshift-security-group-id>
aws ec2 delete-subnet --subnet-id <subnet-id1>
aws ec2 delete-subnet --subnet-id <subnet-id2>
aws ec2 delete-subnet --subnet-id <subnet-id3>
aws ec2 delete-route-table --route-table-id <route-table-id>
aws ec2 detach-internet-gateway --internet-gateway-id <igw-id> --vpc-id <vpc-id>
aws ec2 delete-internet-gateway --internet-gateway-id <igw-id>
aws ec2 delete-vpc --vpc-id <vpc-id>